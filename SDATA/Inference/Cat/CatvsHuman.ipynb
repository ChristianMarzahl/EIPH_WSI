{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import openslide\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.callbacks.csv_logger import CSVLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from object_detection_fastai.helper.object_detection_helper import *\n",
    "from object_detection_fastai.helper.wsi_loader import *\n",
    "from object_detection_fastai.loss.RetinaNetFocalLoss import RetinaNetFocalLoss\n",
    "from object_detection_fastai.models.RetinaNet import RetinaNet\n",
    "from object_detection_fastai.callbacks.callbacks import BBLossMetrics, BBMetrics, PascalVOCMetric, PascalVOCMetricByDistance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slides_train = list(set(['BAL_Cat_Turnbull blue_1.svs', \n",
    "                              'BAL_Cat_Turnbull blue_2.svs', \n",
    "                              'BAL_Cat_Turnbull blue_3.svs',\n",
    "                              'BAL_Cat_Turnbull blue_4.svs', \n",
    "                              'BAL_Cat_Turnbull blue_5.svs', \n",
    "                              #'BAL_Cat_Turnbull blue_6.svs',\n",
    "                              #'BAL_Cat_Turnbull blue_7.svs', \n",
    "                              #'BAL_Cat_Turnbull blue_8.svs', \n",
    "                              #'BAL_Cat_Turnbull blue_9.svs',\n",
    "                              #'BAL_Cat_Turnbull blue_10.svs', \n",
    "                              #'BAL_Cat_Turnbull blue_11.svs',\n",
    "                              #'BAL_Cat_Turnbull blue_12.svs',\n",
    "                              #'BAL_Cat_Turnbull blue_13.svs', \n",
    "                              #'BAL_Cat_Turnbull blue_14.svs'\n",
    "                        ]))\n",
    "\n",
    "slides_val = list(set(['10120_19 humane BAL Turnbull Blau.svs',\n",
    "                              '10277_19 humane BAL Turnbull Blau.svs', \n",
    "                              '10672_19 humane BAL Turnbull Blau.svs']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations_path = Path(\"../../Statistics/EIPH_Annotations.pkl\")\n",
    "annotations = pd.read_pickle(annotations_path)\n",
    "annotations_train = annotations[annotations[\"image_name\"].isin(slides_train)]\n",
    "annotations_val = annotations[annotations[\"image_name\"].isin(slides_val)]\n",
    "annotations_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slides_path = Path(\"../../Slides\")\n",
    "files = {slide.name: slide for slide in slides_path.rglob(\"*.svs\")  if slide.name in slides_train + slides_val}\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 1024 \n",
    "level = 0\n",
    "bs = 16\n",
    "train_images = 2500\n",
    "val_images = 1500\n",
    "experiment_name = \"CatVsHuman\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_files = []\n",
    "val_files = []\n",
    "\n",
    "for image_name in annotations_train[\"image_name\"].unique():\n",
    "    \n",
    "    annotations = annotations_train[annotations_train[\"image_name\"] == image_name]\n",
    "    annotations = annotations[annotations[\"deleted\"] == False]\n",
    "    \n",
    "    slide_path = files[image_name]\n",
    "    labels =  list(annotations[\"grade\"])\n",
    "    bboxes = [[vector[\"x1\"], vector[\"y1\"], vector[\"x2\"], vector[\"y2\"]] for vector in annotations[\"vector\"]]\n",
    "    \n",
    "    train_files.append(SlideContainer(slide_path, y=[bboxes, labels],  level=level, width=size, height=size))\n",
    "    \n",
    "for image_name in annotations_val[\"image_name\"].unique():\n",
    "    \n",
    "    annotations = annotations_val[annotations_val[\"image_name\"] == image_name]\n",
    "    annotations = annotations[annotations[\"deleted\"] == False]\n",
    "    \n",
    "    slide_path = files[image_name]\n",
    "    labels =  list(annotations[\"grade\"])\n",
    "    bboxes = [[vector[\"x1\"], vector[\"y1\"], vector[\"x2\"], vector[\"y2\"]] for vector in annotations[\"vector\"]]\n",
    "    \n",
    "    val_files.append(SlideContainer(slide_path, y=[bboxes, labels],  level=level, width=size, height=size))\n",
    "    \n",
    "train_files = list(np.random.choice(train_files, train_images))\n",
    "valid_files = list(np.random.choice(val_files, val_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfms = get_transforms(do_flip=True,\n",
    "                      flip_vert=True,\n",
    "                      #max_rotate=90,\n",
    "                      max_lighting=0.0,\n",
    "                      max_zoom=1.,\n",
    "                      max_warp=0.0,\n",
    "                      p_affine=0.5,\n",
    "                      p_lighting=0.0,\n",
    "                      #xtra_tfms=xtra_tfms,\n",
    "                     )\n",
    "tfms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_y_func(x):\n",
    "    return x.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train =  ObjectItemListSlide(train_files, path=slides_path)\n",
    "valid = ObjectItemListSlide(valid_files, path=slides_path)\n",
    "item_list = ItemLists(slides_path, train, valid)\n",
    "lls = item_list.label_from_func(get_y_func, label_cls=SlideObjectCategoryList) #\n",
    "lls = lls.transform(tfms, tfm_y=True, size=size)\n",
    "data = lls.databunch(bs=bs, collate_fn=bb_pad_collate, num_workers=0).normalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.show_batch(rows=3, ds_type=DatasetType.Train, figsize=(15,15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchors = create_anchors(sizes=[(32,32)], ratios=[1], scales=[0.6, 0.7, 0.9, 1.25, 1.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crit = RetinaNetFocalLoss(anchors)\n",
    "encoder = create_body(models.resnet18, True, -2)\n",
    "model = RetinaNet(encoder, n_classes=data.train_ds.c, n_anchors=5, sizes=[32], chs=128, final_bias=-4., n_conv=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voc = PascalVOCMetricByDistance(anchors, size, [str(i) for i in data.train_ds.y.classes[1:]], radius=40)\n",
    "learn = Learner(data, model, loss_func=crit, callback_fns=[BBMetrics, partial(CSVLogger, append=True, filename=experiment_name)], #BBMetrics, ShowGraph\n",
    "                metrics=[voc]\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.split([model.encoder[6], model.c5top5])\n",
    "learn.freeze_to(-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.lr_find()\n",
    "learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(3, 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.unfreeze()\n",
    "learn.fit_one_cycle(10, 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(10, 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "stats = {\"anchors\": anchors,\n",
    "         \"mean\": to_np(data.stats[0]),\n",
    "         \"std\": to_np(data.stats[1]),\n",
    "         \"size\": size,\n",
    "         \"n_classes\": 6,\n",
    "         \"n_anchors\": 5,\n",
    "         \"sizes\": [32],\n",
    "         \"chs\": 128,\n",
    "         \"encoder\": \"RN-18\",\n",
    "         \"n_conv\": 3,\n",
    "         \"level\": 0,\n",
    "         \"model\": get_model(learn.model).state_dict()\n",
    "        }\n",
    "\n",
    "torch.save(stats, \"{}.p\".format(experiment_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide_object_result(learn, anchors, detect_thresh=0.3, nms_thresh=0.2, image_count=20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
