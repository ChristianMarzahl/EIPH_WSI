{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import numpy as np\n",
    "from SlideRunner.dataAccess.database import Database\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import openslide\n",
    "from random import randint\n",
    "from Detection.data_loader import *\n",
    "import pickle\n",
    "import uuid\n",
    "import json\n",
    "import cv2\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path('/data/Datasets/EIPH_WSI/')\n",
    "\n",
    "database = Database()\n",
    "database.open(str(path/'EIPH.sqlite'))\n",
    "\n",
    "files = []\n",
    "lbl_bbox = []\n",
    "size = 1024\n",
    "level = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/24 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading DB into memory ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  4%|▍         | 1/24 [00:00<00:06,  3.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading DB into memory ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  8%|▊         | 2/24 [00:01<00:18,  1.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading DB into memory ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 12%|█▎        | 3/24 [00:02<00:15,  1.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading DB into memory ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 5/24 [00:02<00:10,  1.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading DB into memory ...\n",
      "Loading DB into memory ...\n",
      "Loading DB into memory ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 8/24 [00:03<00:06,  2.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading DB into memory ...\n",
      "Loading DB into memory ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 11/24 [00:03<00:04,  2.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading DB into memory ...\n",
      "Loading DB into memory ...\n",
      "Loading DB into memory ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████     | 12/24 [00:03<00:03,  3.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading DB into memory ...\n",
      "Loading DB into memory ...\n",
      "Loading DB into memory ...\n",
      "Loading DB into memory ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████   | 17/24 [00:04<00:01,  3.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading DB into memory ...\n",
      "Loading DB into memory ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 79%|███████▉  | 19/24 [00:04<00:01,  3.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading DB into memory ...\n",
      "Loading DB into memory ...\n",
      "Loading DB into memory ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 88%|████████▊ | 21/24 [00:05<00:00,  3.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading DB into memory ...\n",
      "Loading DB into memory ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:06<00:00,  3.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading DB into memory ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "getslides = \"\"\"SELECT uid, filename FROM Slides\"\"\"\n",
    "for currslide, filename in tqdm(database.execute(getslides).fetchall()):\n",
    "    database.loadIntoMemory(currslide)\n",
    "\n",
    "    check = True if 'erliner' in filename else False\n",
    "    slidetype = 'Berliner Blau/' if check else 'Turnbull Blue/'\n",
    "\n",
    "    slide_path = path / slidetype / filename\n",
    "\n",
    "    slide = openslide.open_slide(str(slide_path))\n",
    "    level = level\n",
    "    level_dimension = slide.level_dimensions[level]\n",
    "    down_factor = slide.level_downsamples[level]\n",
    "\n",
    "    classes = {3: 0, 4: 1, 5: 2, 6: 3, 7: 4}\n",
    "    labels, bboxes = [], []\n",
    "    for id, annotation in database.annotations.items():\n",
    "        if annotation.labels[0].classId in classes:\n",
    "            d = 2 * annotation.r / down_factor\n",
    "            x_min = (annotation.x1 - annotation.r) / down_factor\n",
    "            y_min = (annotation.y1 - annotation.r) / down_factor\n",
    "            x_max = x_min + d\n",
    "            y_max = y_min + d\n",
    "            label = classes[annotation.labels[0].classId]\n",
    "\n",
    "            bboxes.append([int(x_min), int(y_min), int(x_max), int(y_max)])\n",
    "            labels.append(label)\n",
    "\n",
    "    if len(bboxes) > 0:\n",
    "        lbl_bbox.append([bboxes, labels])\n",
    "        files.append(SlideContainer(slide_path, [[0], [1]] ,level, size, size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "img2bbox = dict(zip(files, np.array(lbl_bbox)))\n",
    "get_y_func = lambda o: img2bbox[o]\n",
    "w, h = size, size\n",
    "\n",
    "num_examples_per_image = 500\n",
    "train_files = files[4:]\n",
    "valid_files = files[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_image_with_boxes(file: SlideContainer, boxes, labels, classes, num_examples_per_image):\n",
    "    image_x, image_y = [], []\n",
    "\n",
    "    for i in range(num_examples_per_image):\n",
    "        class_id = np.random.choice(classes, 1)[0]\n",
    "        ids = labels == class_id\n",
    "        xmin, ymin, xmax, ymax = np.array(boxes)[ids][randint(0, np.count_nonzero(ids) - 1)]\n",
    "\n",
    "        x = int(xmin - w / 2)\n",
    "        y = int(ymin - h / 2)\n",
    "\n",
    "        # select_boxes\n",
    "        select_boxes = np.copy(boxes)\n",
    "        select_boxes[:, [0, 2]] = select_boxes[:, [0, 2]] - x\n",
    "        select_boxes[:, [1, 3]] = select_boxes[:, [1, 3]] - y\n",
    "\n",
    "        bb_widths = (select_boxes[:, 2] - select_boxes[:, 0]) / 2\n",
    "        bb_heights = (select_boxes[:, 3] - select_boxes[:, 1]) / 2\n",
    "\n",
    "        ids = ((select_boxes[:, 0] + bb_widths) > 0) \\\n",
    "              & ((select_boxes[:, 1] + bb_heights) > 0) \\\n",
    "              & ((select_boxes[:, 2] - bb_widths) < w) \\\n",
    "              & ((select_boxes[:, 3] - bb_heights) < h)\n",
    "\n",
    "        select_labels = np.copy(labels)[ids]\n",
    "        select_boxes = np.copy(select_boxes)[ids]\n",
    "\n",
    "        patch = file.get_patch(x,y)\n",
    "\n",
    "        image_x.append(patch)\n",
    "        image_y.append([select_boxes, select_labels])\n",
    "\n",
    "    return image_x, image_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_to_feature_dict(image, y, size, image_id, image_format=\"png\"):\n",
    "    boxes, labels = y\n",
    "\n",
    "    filename = image_id\n",
    "\n",
    "    encoded_jpg = cv2.imencode('.{}'.format(image_format), image[:, :, [2,1,0]])[1].tostring()\n",
    "\n",
    "    key = hashlib.sha256(encoded_jpg).hexdigest()\n",
    "\n",
    "    encoded_image_data = encoded_jpg  # Encoded image bytes\n",
    "    image_format = b'png' if image_format == \"png\" else b'jpeg'\n",
    "\n",
    "    xmins = []  # List of normalized left x coordinates in bounding box (1 per box)\n",
    "    xmaxs = []  # List of normalized right x coordinates in bounding box\n",
    "    # (1 per box)\n",
    "    ymins = []  # List of normalized top y coordinates in bounding box (1 per box)\n",
    "    ymaxs = []  # List of normalized bottom y coordinates in bounding box\n",
    "    # (1 per box)\n",
    "    classes_text = []  # List of string class name of bounding box (1 per box)\n",
    "    classes = []  # List of integer class id of bounding box (1 per box)\n",
    "    encoded_mask_png_list = []  # for each rect the mask as png encoded\n",
    "\n",
    "    for box, label in zip(boxes, labels):\n",
    "        classes_text.append(str(label).encode('utf8'))\n",
    "        classes.append(label + 1)\n",
    "\n",
    "        ##calculate BBoxes\n",
    "        x_min = max(0, int(box[0]))\n",
    "        y_min = max(0, int(box[1]))\n",
    "\n",
    "        x_max = min(size, (int(box[2])))\n",
    "        y_max = min(size, (int(box[3])))\n",
    "\n",
    "        xmins.append(float(x_min / size))\n",
    "        ymins.append(float(y_min / size))\n",
    "\n",
    "        xmaxs.append(float(x_max / size))\n",
    "        ymaxs.append(float(y_max / size))\n",
    "\n",
    "    feature_dict = {\n",
    "        'image/height': tf.train.Feature(int64_list=tf.train.Int64List(value=[size])),\n",
    "        'image/width': tf.train.Feature(int64_list=tf.train.Int64List(value=[size])),\n",
    "        'image/filename': tf.train.Feature(bytes_list=tf.train.BytesList(value=[filename.encode('utf8')])),\n",
    "        'image/source_id': tf.train.Feature(bytes_list=tf.train.BytesList(value=[filename.encode('utf8')])),\n",
    "        'image/key/sha256': tf.train.Feature(bytes_list=tf.train.BytesList(value=[key.encode('utf8')])),\n",
    "        'image/encoded': tf.train.Feature(bytes_list=tf.train.BytesList(value=[encoded_image_data])),\n",
    "        'image/format': tf.train.Feature(bytes_list=tf.train.BytesList(value=[image_format])),\n",
    "        'image/object/bbox/xmin': tf.train.Feature(float_list=tf.train.FloatList(value=xmins)),\n",
    "        'image/object/bbox/xmax': tf.train.Feature(float_list=tf.train.FloatList(value=xmaxs)),\n",
    "        'image/object/bbox/ymin': tf.train.Feature(float_list=tf.train.FloatList(value=ymins)),\n",
    "        'image/object/bbox/ymax': tf.train.Feature(float_list=tf.train.FloatList(value=ymaxs)),\n",
    "        'image/object/class/text': tf.train.Feature(bytes_list=tf.train.BytesList(value=classes_text)),\n",
    "        'image/object/class/label': tf.train.Feature(int64_list=tf.train.Int64List(value=classes)),\n",
    "    }\n",
    "\n",
    "    return tf.train.Example(features=tf.train.Features(feature=feature_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_images_to_tfrecord(files, path):\n",
    "\n",
    "    for file in tqdm(files):\n",
    "        boxes, labels = get_y_func(file)\n",
    "        boxes = np.array(boxes)\n",
    "        labels = np.array(labels)\n",
    "        classes = list(set(labels))\n",
    "\n",
    "        x_batch, y_batch = extract_image_with_boxes(file,\n",
    "                                                    boxes,\n",
    "                                                    labels,\n",
    "                                                    classes,\n",
    "                                                    num_examples_per_image)\n",
    "\n",
    "        filename = file.file.stem\n",
    "        writer = tf.python_io.TFRecordWriter(str(path) + \"/\" + filename + \".tfrecord\")\n",
    "\n",
    "        image_id = 0\n",
    "        for image, y in zip(x_batch, y_batch):\n",
    "            tf_example = image_to_feature_dict(image, y, size, filename + \"_\" + str(image_id) + \".png\")\n",
    "            writer.write(tf_example.SerializeToString())\n",
    "\n",
    "            image_id += 1\n",
    "\n",
    "        writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = Path('/data/Datasets/EIPH_WSI/RCNN-Patches/1024_API/train_2/')\n",
    "val_path = Path('/data/Datasets/EIPH_WSI/RCNN-Patches/1024_API/val_2/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "  8%|▊         | 1/13 [02:40<32:02, 160.21s/it]\u001b[A\n",
      " 15%|█▌        | 2/13 [05:34<30:40, 167.33s/it]\u001b[A\n",
      " 23%|██▎       | 3/13 [08:25<28:04, 168.43s/it]\u001b[A\n",
      " 31%|███       | 4/13 [11:27<25:47, 171.91s/it]\u001b[A\n",
      " 38%|███▊      | 5/13 [14:13<22:44, 170.61s/it]\u001b[A\n",
      " 46%|████▌     | 6/13 [17:11<20:03, 171.89s/it]\u001b[A\n",
      " 54%|█████▍    | 7/13 [20:28<17:32, 175.45s/it]\u001b[A\n",
      " 62%|██████▏   | 8/13 [23:50<14:53, 178.80s/it]\u001b[A\n",
      " 69%|██████▉   | 9/13 [26:39<11:50, 177.72s/it]\u001b[A\n",
      " 77%|███████▋  | 10/13 [29:34<08:52, 177.47s/it]\u001b[A\n",
      " 85%|████████▍ | 11/13 [32:50<05:58, 179.10s/it]\u001b[A\n",
      " 92%|█████████▏| 12/13 [35:53<02:59, 179.46s/it]\u001b[A\n",
      "100%|██████████| 13/13 [38:58<00:00, 179.88s/it]\u001b[A\n",
      "\u001b[A"
     ]
    }
   ],
   "source": [
    "convert_images_to_tfrecord(train_files, train_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 25%|██▌       | 1/4 [02:57<08:52, 177.37s/it]\u001b[A\n",
      " 50%|█████     | 2/4 [05:47<05:47, 173.93s/it]\u001b[A\n",
      " 75%|███████▌  | 3/4 [08:39<02:53, 173.30s/it]\u001b[A\n",
      "100%|██████████| 4/4 [11:23<00:00, 170.87s/it]\u001b[A\n",
      "\u001b[A"
     ]
    }
   ],
   "source": [
    "convert_images_to_tfrecord(valid_files, val_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train comands\n",
    "\n",
    "\n",
    "#### Model:\n",
    "export CUDA_VISIBLE_DEVICES=0\n",
    "\n",
    "python /home/c.marzahl@de.eu.local/ProgProjekte/Demo/models/research/object_detection/legacy/train.py --logtostderr --pipeline_config_path=pipeline.config --train_dir=train/baseline\n",
    "\n",
    "#### Eval:\n",
    "export CUDA_VISIBLE_DEVICES=1\n",
    "\n",
    "python /home/c.marzahl@de.eu.local/ProgProjekte/Demo/models/research/object_detection/legacy/eval.py --logtostderr --pipeline_config_path=pipeline.config --checkpoint_dir=train/baseline --eval_dir=eval/baseline\n",
    "\n",
    "#### Tensorboard:\n",
    "tensorboard --logdir=./ --port=6007\n",
    "\n",
    "#### Compile:\n",
    "python /home/c.marzahl@de.eu.local/ProgProjekte/Demo/models/research/object_detection/export_inference_graph.py --input_type image_tensor --pipeline_config_path=pipeline.config --trained_checkpoint_prefix=model.ckpt-20000 --output_directory=inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
